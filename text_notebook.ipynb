{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.9.11-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------  41.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------  41.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------  41.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------  41.0/41.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 41.5/41.5 kB 100.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2024.9.11-cp311-cp311-win_amd64.whl (274 kB)\n",
      "   ---------------------------------------- 0.0/274.0 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 20.5/274.0 kB ? eta -:--:--\n",
      "   ------- ------------------------------- 51.2/274.0 kB 525.1 kB/s eta 0:00:01\n",
      "   ------- ------------------------------- 51.2/274.0 kB 525.1 kB/s eta 0:00:01\n",
      "   ------- ------------------------------- 51.2/274.0 kB 525.1 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 112.6/274.0 kB 386.4 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 112.6/274.0 kB 386.4 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 153.6/274.0 kB 399.3 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 184.3/274.0 kB 446.4 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 204.8/274.0 kB 415.7 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.0 kB 462.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.0 kB 462.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.0 kB 462.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.0 kB 462.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.0 kB 462.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.0 kB 462.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.0 kB 462.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.0 kB 462.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.0 kB 462.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.0 kB 462.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.0 kB 462.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.0 kB 462.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.0 kB 462.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.0 kB 462.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 266.2/274.0 kB 215.7 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 266.2/274.0 kB 215.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- 274.0/274.0 kB 211.0 kB/s eta 0:00:00\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.9.11\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk,pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(file_link):\n",
    "    \"\"\"\n",
    "    Loads data from a specified file link or local path and returns it as a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        file_link: The link (URL or local path) to the file. The file can be in JSON, CSV, or Excel format.\n",
    "    \n",
    "    Returns:\n",
    "        data: A pandas DataFrame containing the loaded data. If the file is invalid or cannot be loaded, it returns None.\n",
    "    \"\"\"\n",
    "    valid_local_drives = ('D:', 'C:', 'E:')  # Local drive letters to check for file paths\n",
    "    valid_extensions = ('.json', '.csv', '.xlsx')  # Valid file extensions\n",
    "\n",
    "    # Check if the file link is a URL\n",
    "    if file_link.startswith('https') and file_link.endswith(valid_extensions):\n",
    "        try:\n",
    "            # Load the data based on its extension\n",
    "            if file_link.endswith('.json'):\n",
    "                data = pd.read_json(file_link, typ='series')\n",
    "            elif file_link.endswith('.csv'):\n",
    "                data = pd.read_csv(file_link)\n",
    "            elif file_link.endswith('.xlsx'):\n",
    "                data = pd.read_excel(file_link)\n",
    "            else:\n",
    "                print(\"Unsupported file format.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file from URL: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Check if the file link is a local file path\n",
    "    elif file_link.startswith(valid_local_drives) and file_link.endswith(valid_extensions):\n",
    "        try:\n",
    "            # Load the data based on its extension\n",
    "            if file_link.endswith('.json'):\n",
    "                data = pd.read_json(file_link, typ='series')\n",
    "            elif file_link.endswith('.csv'):\n",
    "                data = pd.read_csv(file_link)\n",
    "            elif file_link.endswith('.xlsx'):\n",
    "                data = pd.read_excel(file_link)\n",
    "            else:\n",
    "                print(\"Unsupported file format.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file from local path: {e}\")\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        print('Invalid link or file path. Please provide a valid link or path.')\n",
    "\n",
    "    # If the data is a Series, convert it to a DataFrame\n",
    "    if isinstance(data, pd.Series):\n",
    "        data = data.to_frame('Values')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Example usage\n",
    "link = input('''Enter the file link (e.g., URL or local path): https://raw.githubusercontent.com/Devian158/AI-Internship-Task/main/Dataset.json\n",
    "''')\n",
    "\n",
    "data = load_data(link)\n",
    "\n",
    "# Display the first few rows of the DataFrame if data is successfully loaded\n",
    "if data is not None:\n",
    "    data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pdf1</th>\n",
       "      <td>https://digiscr.sci.gov.in/pdf_viewer?dir=YWRt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pdf2</th>\n",
       "      <td>https://digiscr.sci.gov.in/pdf_viewer?dir=YWRt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pdf3</th>\n",
       "      <td>https://cdnbbsr.s3waas.gov.in/s380537a945c7aaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pdf4</th>\n",
       "      <td>https://www.mha.gov.in/sites/default/files/250...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pdf5</th>\n",
       "      <td>https://rbidocs.rbi.org.in/rdocs/PressRelease/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Values\n",
       "pdf1  https://digiscr.sci.gov.in/pdf_viewer?dir=YWRt...\n",
       "pdf2  https://digiscr.sci.gov.in/pdf_viewer?dir=YWRt...\n",
       "pdf3  https://cdnbbsr.s3waas.gov.in/s380537a945c7aaa...\n",
       "pdf4  https://www.mha.gov.in/sites/default/files/250...\n",
       "pdf5  https://rbidocs.rbi.org.in/rdocs/PressRelease/..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and saved https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTUwL3ZvbHVtZSAxL1BhcnQgSS9Db21taXNzaW9uZXIgb2YgSW5jb21lIFRheCwgV2VzdCBCZW5nYWxfQ2FsY3V0dGEgQWdlbmN5IEx0ZC5fMTY5NzYwNjMxMC5wZGY= as downloaded_data\\pdf1.pdf\n",
      "Downloaded and saved https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTUyL3ZvbHVtZSAxL1BhcnQgSS90aGUgc3RhdGUgb2YgYmloYXJfbWFoYXJhamFkaGlyYWphIHNpciBrYW1lc2h3YXIgc2luZ2ggb2YgZGFyYmhhbmdhIGFuZCBvdGhlcnNfMTY5ODMxODQ0OC5wZGY= as downloaded_data\\pdf2.pdf\n",
      "Downloaded and saved https://cdnbbsr.s3waas.gov.in/s380537a945c7aaa788ccfcdf1b99b5d8f/uploads/2024/07/20240716890312078.pdf as downloaded_data\\pdf3.pdf\n",
      "Downloaded and saved https://www.mha.gov.in/sites/default/files/250883_english_01042024.pdf as downloaded_data\\pdf4.pdf\n",
      "Downloaded and saved https://rbidocs.rbi.org.in/rdocs/PressRelease/PDFs/PR60974A2ED1DFDB84EC0B3AABFB8419E1431.PDF as downloaded_data\\pdf5.pdf\n",
      "Downloaded and saved https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS90aGUgdGF0YSBvaWwgbWlsbHMgY28uIGx0ZC5faXRzIHdvcmttZW4gYW5kIG90aGVyc18xNjk5MzMzODYyLnBkZg== as downloaded_data\\pdf6.pdf\n",
      "Downloaded and saved https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS9ncmVhdCBpbmRpYW4gbW90b3Igd29ya3MgbHRkLiwgYW5kIGFub3RoZXJfdGhlaXIgZW1wbG95ZWVzIGFuZCBvdGhlcnNfMTY5OTMzNjM1NS5wZGY= as downloaded_data\\pdf7.pdf\n",
      "Downloaded and saved https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS9tZXNzcnMuIGlzcGFoYW5pIGx0ZC4gY2FsY3V0dGFfaXNwYWhhbmkgZW1wbG95ZWVzICB1bmlvbl8xNjk5MzM4NTQ5LnBkZg== as downloaded_data\\pdf8.pdf\n",
      "Downloaded and saved https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS9waHVsYmFyaSB0ZWEgZXN0YXRlX2l0cyB3b3JrbWVuXzE2OTkzMzkyMjYucGRm as downloaded_data\\pdf9.pdf\n",
      "Downloaded and saved https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS90aGUgbG9yZCBrcmlzaG5hIHN1Z2FyIG1pbGxzIGx0ZC4sIGFuZCBhbm90aGVyX3RoZSB1bmlvbiBvZiBpbmRpYSBhbmQgYW5vdGhlcl8xNjk5MzQxMDE0LnBkZg== as downloaded_data\\pdf10.pdf\n",
      "Downloaded and saved https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS8xNjk5NTIxMzUwLnBkZg== as downloaded_data\\pdf11.pdf\n",
      "Downloaded and saved https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS90aGUgZ3JhaGFtIHRyYWRpbmcgY28uIChpbmRpYSkgbHRkLl9pdHMgd29ya21lbl8xNjk5NTIzNTc3LnBkZg== as downloaded_data\\pdf12.pdf\n",
      "Downloaded and saved https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS90aGUgY29tbWlzc2lvbmVyIG9mIGluY29tZS10YXgsIGJvbWJheV9yYW5jaGhvZGRhcyBrYXJzb25kYXMsIGJvbWJheV8xNjk5NTI2MjI3LnBkZg== as downloaded_data\\pdf13.pdf\n",
      "Downloaded and saved https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS8xNjk5NTI2ODA0LnBkZg== as downloaded_data\\pdf14.pdf\n",
      "Downloaded and saved https://digiscr.sci.gov.in/pdf_viewer?dir=YWRtaW4vanVkZ2VtZW50X2ZpbGUvanVkZ2VtZW50X3BkZi8xOTYwL3ZvbHVtZSAxL1BhcnQgSS9zaHJpIGIuIHAuIGhpcmEsIHdvcmtzIG1hbmFnZXIsIGNlbnRyYWwgcmFpbHdheSwgcGFyZWwsIGJvbWJheSBldGMuX3NocmkgYy4gbS4gcHJhZGhhbiBldGMuXzE2OTk1MjcyMTcucGRm as downloaded_data\\pdf15.pdf\n",
      "Downloaded and saved https://www.sebi.gov.in/sebi_data/attachdocs/1292585113260.pdf as downloaded_data\\pdf16.pdf\n",
      "Error downloading https://ijtr.nic.in/Circular%20Orders%20(Supplement).pdf: HTTPSConnectionPool(host='ijtr.nic.in', port=443): Max retries exceeded with url: /Circular%20Orders%20(Supplement).pdf (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')))\n",
      "Downloaded and saved https://enforcementdirectorate.gov.in/sites/default/files/Act%26rules/The%20Prevention%20of%20Money-laundering%20%28Maintenance%20of%20Records%29%20Rules%2C%202005.pdf as downloaded_data\\pdf18.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def download_and_store(data, folder_name):\n",
    "    \"\"\"\n",
    "    Downloads all data from the links in the DataFrame and stores them in a specified folder.\n",
    "    \n",
    "    Args:\n",
    "        data: A pandas DataFrame with a column containing the links to download.\n",
    "        folder_name: The name of the folder where the downloaded data will be stored.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the folder if it does not exist\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    # Iterate over each row in the DataFrame to get the links\n",
    "    for index, row in data.iterrows():\n",
    "        link = row['Values']  #the download links are stored in the 'Values' column\n",
    "        \n",
    "        try:\n",
    "            # Send an HTTP GET request to download the file from the link\n",
    "            response = requests.get(link, stream=True)\n",
    "            \n",
    "            # Check if the request was successful (HTTP status code 200)\n",
    "            if response.status_code == 200:\n",
    "                # Construct the file name and path to store the downloaded file\n",
    "                file_name = os.path.join(folder_name, f\"{index}.pdf\")  # Assuming the files are PDFs. Adjust as needed.\n",
    "                \n",
    "                # Write the content of the file in chunks to avoid memory issues with large files\n",
    "                with open(file_name, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                print(f\"Downloaded and saved {link} as {file_name}\")\n",
    "            else:\n",
    "                # If the request fails, log the status code\n",
    "                print(f\"Failed to download {link} (Status code: {response.status_code})\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # If there is any error during the download, print the error message\n",
    "            print(f\"Error downloading {link}: {e}\")\n",
    "\n",
    "# Specify the name of the folder where the downloaded files will be stored\n",
    "folder_name = \"downloaded_data\"\n",
    "\n",
    "# Download and store all data from the DataFrame\n",
    "download_and_store(data, folder_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from pymongo import MongoClient\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_pdf_files(folder_path):\n",
    "    \"\"\"\n",
    "    Retrieve all PDF files from a given folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing PDF files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of file paths for all PDF files in the folder.\n",
    "    \"\"\"\n",
    "    pdf_files = [\n",
    "        os.path.join(folder_path, file)\n",
    "        for file in os.listdir(folder_path)\n",
    "        if file.endswith('.pdf')\n",
    "    ]\n",
    "    \n",
    "    return pdf_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the extracted text (str) and the number of pages (int).\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    \n",
    "    # Open the PDF file using pdfplumber\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        num_pages = len(pdf.pages)  # Get the total number of pages\n",
    "        \n",
    "        # Extract text from each page\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() or \"\"  # Handle pages with no text\n",
    "\n",
    "    return text, num_pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "def custom_summarization(text, num_sentences=3):\n",
    "    \"\"\"\n",
    "    Summarize the text by selecting the top N ranked sentences.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to summarize.\n",
    "        num_sentences (int): The number of sentences to include in the summary (default is 3).\n",
    "\n",
    "    Returns:\n",
    "        str: A summary composed of the top-ranked sentences.\n",
    "    \"\"\"\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # Create a frequency distribution of words\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    # Rank sentences based on the sum of word frequencies\n",
    "    ranked_sentences = sorted(\n",
    "        sentences,\n",
    "        key=lambda sentence: sum(word_freq[word] for word in word_tokenize(sentence.lower())),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    # Select the top N ranked sentences for the summary\n",
    "    summary = \" \".join(ranked_sentences[:num_sentences])\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Remove stop words from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text from which to remove stop words.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with stop words removed.\n",
    "    \"\"\"\n",
    "    # Create a set of English stop words for faster lookup\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Split the text into words and filter out stop words\n",
    "    filtered_words = [\n",
    "        word for word in text.split() if word.lower() not in stop_words\n",
    "    ]\n",
    "    \n",
    "    # Join the filtered words back into a single string\n",
    "    return \" \".join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_keywords(text, top_n=5):\n",
    "    \"\"\"\n",
    "    Extract top N keywords based on frequency.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The input text from which to extract keywords.\n",
    "    top_n (int): The number of top keywords to return. Default is 5.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of top N keywords.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stopwords and convert text to lowercase\n",
    "    text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Calculate word frequencies\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Return top N keywords that are alphabetic\n",
    "    return [word for word, freq in word_freq.most_common(top_n) if word.isalpha()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def save_to_mongo(pdf_path, summary, keywords):\n",
    "    \"\"\"\n",
    "    Save the processed data to MongoDB.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "        summary (str): The summarized text of the PDF.\n",
    "        keywords (list): A list of keywords associated with the PDF.\n",
    "    \"\"\"\n",
    "    # Extract the PDF name from the file path without the extension\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    \n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client['pdf_summarization53']\n",
    "    collection = db['summaries']\n",
    "    \n",
    "    # Insert the data into the collection\n",
    "    collection.insert_one({\n",
    "        'pdf_name': pdf_name,\n",
    "        'summary': summary,\n",
    "        'keywords': keywords\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Process a single PDF file to extract text, summarize it, and save the data to MongoDB.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file to process.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract text and number of pages from the PDF file\n",
    "        text, num_pages = extract_text_from_pdf(pdf_path)\n",
    "        \n",
    "        # Determine the number of sentences for the summary based on the number of pages\n",
    "        if num_pages <= 10:\n",
    "            summary = custom_summarization(text, num_sentences=3)  # For PDFs with 10 or fewer pages, summarize with 3 sentences\n",
    "        elif 10 < num_pages <= 30:\n",
    "            summary = custom_summarization(text, num_sentences=5)  # For PDFs with 11 to 30 pages, summarize with 5 sentences\n",
    "        else:\n",
    "            summary = custom_summarization(text, num_sentences=10)  # For PDFs with more than 30 pages, summarize with 10 sentences\n",
    "        \n",
    "        # Extract keywords from the text\n",
    "        keywords = extract_keywords(text)\n",
    "        \n",
    "        # Save the PDF name, summary, and keywords to MongoDB\n",
    "        save_to_mongo(pdf_path, summary, keywords)\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle exceptions that may occur during processing and print an error message\n",
    "        print(f\"Error processing {pdf_path}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_pdf_concurrently(pdf_paths):\n",
    "    \"\"\"\n",
    "    Process multiple PDF files concurrently using multithreading.\n",
    "\n",
    "    Args:\n",
    "        pdf_paths (list): A list of paths to PDF files to process.\n",
    "    \"\"\"\n",
    "    # Create a ThreadPoolExecutor to manage a pool of threads for concurrent processing\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # Map the process_single_pdf function to the list of PDF paths\n",
    "        # This will execute process_single_pdf for each PDF path concurrently\n",
    "        executor.map(process_single_pdf, pdf_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(folder_path):\n",
    "    \"\"\"Main pipeline to process all PDFs in a folder.\"\"\"\n",
    "    pdf_paths = get_pdf_files(folder_path)\n",
    "    process_pdf_concurrently(pdf_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline (Provide the folder path)\n",
    "n=r'D:\\Deep learning model\\nlp Projects\\text\\downloaded_data'\n",
    "run_pipeline(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/12ashwani/text_pipeline.git"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
